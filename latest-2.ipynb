{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f5ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 0.],\n",
      "        [2., 0., 4.],\n",
      "        [0., 4., 0.]], requires_grad=True)\n",
      "tensor([[2., 0., 0.],\n",
      "        [0., 6., 0.],\n",
      "        [0., 0., 4.]], grad_fn=<DiagBackward0>)\n",
      "tensor([-1.4901e-08,  1.0000e+00,  2.0000e+00], grad_fn=<LinalgEighBackward0>)\n",
      "tensor([[0., 2., 0.],\n",
      "        [2., 0., 4.],\n",
      "        [0., 4., 0.]], requires_grad=True)\n",
      "tensor([ 8.1650e-01, -5.1165e-08, -5.7735e-01], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.1921e-07, 5.0731e-01, 1.1921e-07], grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ddn.pytorch.node import *\n",
    "\n",
    "class NormalizedCuts(EqConstDeclarativeNode):\n",
    "    \"\"\"\n",
    "    A declarative node to embed Normalized Cuts into a Neural Network\n",
    "    \n",
    "    Normalized Cuts and Image Segmentation https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf\n",
    "    Shi, J., & Malik, J. (2000)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.chunk_size = 1\n",
    "        \n",
    "    def objective(self, x, y):\n",
    "        \"\"\"\n",
    "        f(W,y) = y^T * (D-W) * y / y^T * D * y\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in len(x):\n",
    "            # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "            D = x[i,...].sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "            L = D - x\n",
    "\n",
    "            y_t = torch.t(y)\n",
    "\n",
    "        return torch.div(torch.matmul(torch.matmul(y_t, L),y),torch.matmul(torch.matmul(y_t,D),y))\n",
    "    \n",
    "    def equality_constraints(self, x, y):\n",
    "        \"\"\"\n",
    "        subject to y^T * D * 1 = 0\n",
    "        \"\"\"\n",
    "        # Ensure correct size and shape of y... scipy minimise flattens y         \n",
    "        N = x.size(dim=0)\n",
    "        \n",
    "        #x is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "        D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "        ONE = torch.ones(N,1)   # Nx1 vector of all ones\n",
    "        y_t = torch.t(y)\n",
    "        \n",
    "        \n",
    "        return torch.matmul(torch.matmul(y_t,D), ONE)\n",
    "\n",
    "    def solve(self, W):\n",
    "        D = torch.diag(torch.sum(W, 0))\n",
    "        D_half_inv = torch.diag(1.0 / torch.sqrt(torch.sum(W, 0)))\n",
    "        M = torch.matmul(D_half_inv, torch.matmul((D - W), D_half_inv))\n",
    "\n",
    "        # M is the normalised laplacian\n",
    "\n",
    "        (w, v) = torch.linalg.eigh(M)\n",
    "\n",
    "        print(W)\n",
    "        print(D)\n",
    "        print(w)\n",
    "        #find index of second smallest eigenvalue\n",
    "        index = torch.argsort(w)[1]\n",
    "\n",
    "        v_partition = v[:, index]\n",
    "        # instead of the sign of a digit being the binary split, let the NN learn it\n",
    "        # v_partition = torch.sign(v_partition)\n",
    "    \n",
    "        # return the eigenvector and a blank context\n",
    "        return v_partition, _\n",
    "    \n",
    "    def test_nobatch(self, x, y=y):\n",
    "        \"\"\"\n",
    "        Test the function, without any batches present\n",
    "        \"\"\"\n",
    "        # Evaluate objective function at (xs,y):\n",
    "        f = torch.enable_grad()(self.objective)(x, y=y) # b\n",
    "\n",
    "        # Compute partial derivative of f wrt y at (xs,y):\n",
    "        fY = grad(f, y, grad_outputs=torch.ones_like(f), create_graph=True)\n",
    "        return fY\n",
    "    \n",
    "node = NormalizedCuts()\n",
    "x_nobatch = torch.tensor([[0,2,0], [2,0,4], [0,4,0]], dtype=torch.float, requires_grad=True)\n",
    "y_nobatch, misc = node.solve(x_nobatch)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# node.gradient(x,y=y)\n",
    "node.test_nobatch(x_nobatch,y=y_nobatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_22578/3196724601.py\u001b[0m(54)\u001b[0;36msolve\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     52 \u001b[0;31m        \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m        \u001b[0mD_half_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 54 \u001b[0;31m        \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_half_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_half_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     56 \u001b[0;31m        \u001b[0;31m# M is the normalised laplacian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x\n",
      "tensor([[0., 2., 0.],\n",
      "        [2., 0., 4.],\n",
      "        [0., 4., 0.]], requires_grad=True)\n",
      "ipdb> a = torch.tensor([[[0,2,0], [2,0,4], [0,4,0]]], dtype=torch.float, requires_grad=True)\n",
      "self = <__main__.NormalizedCuts object at 0x7f48b2929d10>\n",
      "W = tensor([[[0., 2., 0.],\n",
      "         [2., 0., 4.],\n",
      "         [0., 4., 0.]]], requires_grad=True)\n",
      "ipdb> a[0]\n",
      "self = <__main__.NormalizedCuts object at 0x7f48b2929d10>\n",
      "W = tensor([[[0., 2., 0.],\n",
      "         [2., 0., 4.],\n",
      "         [0., 4., 0.]]], requires_grad=True)\n",
      "ipdb> a[0][0]\n",
      "self = <__main__.NormalizedCuts object at 0x7f48b2929d10>\n",
      "W = tensor([[[0., 2., 0.],\n",
      "         [2., 0., 4.],\n",
      "         [0., 4., 0.]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335cc7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 0.],\n",
      "        [2., 0., 4.],\n",
      "        [0., 4., 0.]], dtype=torch.float64)\n",
      "tensor([ 8.1650e-01,  6.1835e-17, -5.7735e-01], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# # Starting again, trying to solve it all...\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.autograd as grad\n",
    "        \n",
    "# def objective(x, y):\n",
    "#     \"\"\"\n",
    "#     f(W,y) = y^T * (D-W) * y / y^T * D * y\n",
    "#     \"\"\"\n",
    "#     # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "#     D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "#     L = D - x\n",
    "\n",
    "#     y_t = torch.t(y)\n",
    "\n",
    "#     return torch.div(torch.matmul(torch.matmul(y_t, L),y),torch.matmul(torch.matmul(y_t,D),y))\n",
    "\n",
    "# def equality_constraints(x, y):\n",
    "#     \"\"\"\n",
    "#     subject to y^T * D * 1 = 0\n",
    "#     \"\"\"\n",
    "#     # Ensure correct size and shape of y... scipy minimise flattens y         \n",
    "#     N = x.size(dim=0)\n",
    "\n",
    "#     #x is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "#     D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "#     ONE = torch.ones(N,1)   # Nx1 vector of all ones\n",
    "#     y_t = torch.t(y)\n",
    "\n",
    "\n",
    "#     return torch.matmul(torch.matmul(y_t,D), ONE)\n",
    "\n",
    "# def solve(W):\n",
    "#     D = torch.diag(torch.sum(W, 0))\n",
    "#     D_half_inv = torch.diag(1.0 / torch.sqrt(torch.sum(W, 0)))\n",
    "#     M = torch.matmul(D_half_inv, torch.matmul((D - W), D_half_inv))\n",
    "\n",
    "#     # M is the normalised laplacian\n",
    "\n",
    "#     (w, v) = torch.linalg.eigh(M)\n",
    "\n",
    "#     #find index of second smallest eigenvalue\n",
    "#     index = torch.argsort(w)[1]\n",
    "\n",
    "#     v_partition = v[:, index]\n",
    "#     # instead of the sign of a digit being the binary split, let the NN learn it\n",
    "#     # v_partition = torch.sign(v_partition)\n",
    "\n",
    "#     # return the eigenvector and a blank context\n",
    "#     return v_partition, _\n",
    "    \n",
    "# x = torch.tensor([[0,2,0], [2,0,4], [0,4,0]]).double()\n",
    "# y, ctx = solve(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f = torch.enable_grad()(self.objective)(*xs, y=y) # b\n",
    "\n",
    "# # Compute partial derivative of f wrt y at (xs,y):\n",
    "# fY = grad(f, y, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
    "# fY = torch.enable_grad()(fY.reshape)(self.b, -1) # bxm\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddn",
   "language": "python",
   "name": "ddn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
