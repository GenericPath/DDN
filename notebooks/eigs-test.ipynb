{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal\n",
    "- Find the smallest eigenvector $\\lambda$ for the NC node\n",
    "\n",
    "Problem\n",
    "- Some methods too slow and/or crash on certain cases\n",
    "---\n",
    "Input\n",
    " - Positive definite matrix (symmetric)\n",
    "\n",
    "Output\n",
    " - Graph cut of the matrix (corresponds to the smallest eigenvector)\n",
    "---\n",
    "Must\n",
    "- Handle image sized inputs\n",
    "- Have reproducible results (fixed seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500,)\n",
      "torch.Size([1, 2500, 2500])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import torch\n",
    "## TODO: Begin with a simple 'image' that has a known cut, this way it can be tested if correct\n",
    "## May be able to compare to sklearn.cluster.SpectralClustering (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html?highlight=lobpcg#r5f6cbeb1558e-4)\n",
    "\n",
    "seed = 123\n",
    "dtype = np.float64 # Currently doesn't do anything as random.rand doesn't accept\n",
    "n = 50\n",
    "# and possibly generate different sparisities?? As realistically will be somewhat sparse\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "# Random image of size\n",
    "input = np.random.randint(0,255, n*n)\n",
    "\n",
    "# TODO: Test for larger sized (full correctly formed, symmetric pos def - but random, and fully random matricies)\n",
    "# TODO: Test against multiple different inputs (list of inputs) and also plot the times, and outputs of each to see if they are correct / consistent\n",
    "\n",
    "# pretend simple image\n",
    "# input = np.array([1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,255,255,255,255,255,255,\n",
    "#                   1,1,1,1,1,255,255,255,255,255,\n",
    "#                   1,1,1,1,1,255,255,255,255,255,\n",
    "#                   1,1,1,1,1,1,255,255,255,255,\n",
    "#                   1,1,1,1,1,1,1,1,255,255,])\n",
    "\n",
    "# in = n random numbers between 0 and 255 # probably better if its slightly realistic?\n",
    "A = linalg.fiedler(input)\n",
    "\n",
    "# TODO: Change this with a real image, and real values from it... \n",
    "#       but for now should be fine (same properties being symmetric positive semi-definite)\n",
    "\n",
    "A = torch.from_numpy(A)\n",
    "A = A.reshape(1,n*n,n*n)\n",
    "\n",
    "# print(A)\n",
    "print(input.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to https://gist.github.com/denis-bz/6a9d7379c8edf965b0a997c2ec2471e1\n",
    "\n",
    "# used to store each of the functions, and allow them to all take the single arg input\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "# scipy and numpy are used for the eigensolvers\n",
    "import scipy\n",
    "\n",
    "# TODO: reinstall scikit-sprase on mac to get it to install properly (wrong depecencies and unknown fix for mac)\n",
    "# until then just dont test unless on debian system\n",
    "sksparse_present = True\n",
    "try:\n",
    "    import sksparse \n",
    "except:\n",
    "    sksparse_present = False\n",
    "\n",
    "\n",
    "# TODO: use the initial vector :)\n",
    "# TODO: and use it such that you test both v0 set to 0 and v0 set to random :) to see if any differences\n",
    "v0 = np.zeros_like(A) # initialise the initial vector to all zeros :)\n",
    "# TODO: Switch v0 to something like X = rng.normal(size=(n, 3)) so normally distributed around origin\n",
    "\n",
    "\n",
    "rcond = 1e-6 # cutoff for small singular values of a (0 if smaller than rcond * largest singular of a)\n",
    "\n",
    "d = A.sum(1) # 1 because 0 is batch\n",
    "D = torch.diag_embed(d) # D = matrix with d on diagonal\n",
    "D = D[0] # Takes form b,N*N,N*N but needs to be non batch for non batch eigensolvers\n",
    "\n",
    "\n",
    "eigs_options = OrderedDict(\n",
    "    ## NOTE: Will need to set the v0 to something consistent\n",
    "    ## NOTE: and if neccessary any seeds used by them....\n",
    "    ## NOTE: All inputs will be positive definite so should be easy :)\n",
    "    \n",
    "    \n",
    "    # Types to try:\n",
    "    # - shift invert (as we are looking for smallest) (https://gist.github.com/denis-bz/2658f671cee9396ac15cfe07dcc6657d)\n",
    "    # - Power iteration, QR, LOBPCG\n",
    "    # - Lanzcos, Arnoldi\n",
    "    # - cholmod (https://scikit-sparse.readthedocs.io/en/latest/cholmod.html, https://stackoverflow.com/questions/59416098/finding-smallest-eigenvectors-of-large-sparse-matrix-over-100x-slower-in-scipy)\n",
    "    # - any gpu based ones? (pytorch perhaps?)\n",
    "\n",
    "    # Numpy (no params only inputs)\n",
    "    np_eig = np.linalg.eig,\n",
    "    np_eigh = np.linalg.eigh, # Should be good\n",
    "    np_eigvals = np.linalg.eigvals,\n",
    "    \n",
    "    # Some parameters for scipy variants\n",
    "    sp_eig = partial(scipy.linalg.eig, check_finite=False), # No extra params\n",
    "\n",
    "    sp_eigh_base = partial(scipy.linalg.eigh), # Should be good (and all variations might improve for the single eigenvector case)\n",
    "    sp_eigh_cff = partial(scipy.linalg.eigh, check_finite=False),\n",
    "    # Subset by index only for evr, evx, and gvx\n",
    "    # Subsetting (0,1] so that [1] gives the second smallest (so indexing is the same as others which return all vectors)\n",
    "    # [1,1] will give just the second smallest\n",
    "    sp_eigh_ss = partial(scipy.linalg.eigh, check_finite=False, subset_by_index=[0,1]), # driver=, type=(generalized or not), \n",
    "    \n",
    "    sp_eigh_ev_ss = partial(scipy.linalg.eigh, check_finite=False, driver='ev'), # symmetric qr, slow but robust\n",
    "    sp_eigh_evd = partial(scipy.linalg.eigh, check_finite=False, driver='evd'), # uses more memory but faster\n",
    "    sp_eigh_evr_ss = partial(scipy.linalg.eigh, check_finite=False, subset_by_index=[0,1], driver='evr'), # optimal for most\n",
    "    sp_eigh_evx_ss = partial(scipy.linalg.eigh, check_finite=False, subset_by_index=[0,1], driver='evx'), # subsets\n",
    "    \n",
    "    # uses D for the generalized problem\n",
    "    sp_eigh_gv = partial(scipy.linalg.eigh, check_finite=False, driver='gv', b=D),\n",
    "    sp_eigh_gvd = partial(scipy.linalg.eigh, check_finite=False, driver='gvd', b=D),\n",
    "    sp_eigh_gvx_ss = partial(scipy.linalg.eigh, check_finite=False, subset_by_index=[0,1], driver='gvx', b=D),\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sparse boys, often more complex may include a v0 and maxiters etc\n",
    "    # use \n",
    "    # TODO: based on the gist implement fancy variations of these (inclue differences in max iters etc..)\n",
    "    # TODO: and utuilise sigma to to the inverted faster thing :)\n",
    "    # sp_sparse_eigs = partial(scipy.sparse.linalg.eigs, k=1),   #  maxiter=max_iter, tol=0, which='SM', k=1)\n",
    "    # sp_sparse_eigsh = partial(scipy.sparse.linalg.eigsh, k=1),\n",
    "    \n",
    "    # Slightly less useful ones?\n",
    "    sp_sparse_lobpcg = partial(scipy.sparse.linalg.lobpcg(), X=v0, largest=False, maxiter=40), # TODO: Verify if n, works instead of n,1\n",
    "    sp_sparse_lobpcg_2 = partial(scipy.sparse.linalg.lobpcg(), X=v0, B=D, largest=False, maxiter=40), # TODO: Could test using constraints, maxiter, tol\n",
    "    sp_sparse_bicg = partial(scipy.sparse.linalg.bicg(), b=D), # or bicgstab? TODO: add params for x0, tol, atol, maxiter maybe\n",
    "    sp_sparse_gmres = partial(scipy.sparse.linalg.gmres(), b=D), # TODO: add params for x0, tol, atol, maxiter maybe\n",
    "    # sp_sparse_splu = partial(scipy.sparse.linalg.splu()), # Only works on sparse matricies\n",
    "    \n",
    "    np_qr = np.linalg.qr(), # TODO: Idk if the outputs of this will match the others... maybe test by hand\n",
    "    np_cholesky = np.linalg.cholesky(), # Outputs only one value, so should work?\n",
    "    # sp_qr = partial(scipy.linalg.qr(), check_finite=False), # TODO: Check outputs of this by hand and if they match\n",
    "    sp_cholesky = partial(scipy.linalg.cholesky(), check_finite=False),\n",
    "\n",
    "    # Weirdo Ones\n",
    "    # TODO: they will return the wrong order of stuff.. so will manually check and test?\n",
    "    # np_lstsq = partial( np.linalg.lstsq, b=D, rcond=rcond ),\n",
    "    # sp_lstsq = partial( scipy.linalg.lstsq, b=D, cond=rcond ),\n",
    "    # np_solve = partial( np.linalg.solve, b=D ),\n",
    "    # sp_solve = partial( scipy.linalg.solve, b=D ),\n",
    "    # np_svd = partial( np.linalg.svd, compute_uv=False ),  # gesdd\n",
    "    # sp_svd = partial( scipy.linalg.svd, compute_uv=False ),  # lapack_driver : {'gesdd', 'gesvd'}\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# scikit-sparse doesn't install nicely on MacOS (but does nicely on debian) so may not be present to test\n",
    "if sksparse_present:\n",
    "    eigs_options['sks_cholmod_cholesky'] = sksparse.cholmod.cholesky # Should be good? (TODO: Verify general eigen vs none)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_eig              :    11 sec | \tsolution = 0.00000 | \teqconst = -528819378.00000\n",
      "np_eigh             :     3 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "np_eigvals          :     6 sec | \tsolution = 0.05049 | \teqconst = -5598531181924992.00000\n",
      "scipy_eig           :    11 sec | \tsolution = -0.00000 | \teqconst = -528819378.00000\n",
      "scipy_eigh_base     :    12 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_cff      :    11 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_ss       :     2 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_ev_ss    :    15 sec | \tsolution = 1.00000 | \teqconst = -0.00000\n",
      "scipy_eigh_evd      :     3 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_evr_ss   :     2 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_evx_ss   :     2 sec | \tsolution = 1.00000 | \teqconst = 0.00000\n",
      "scipy_eigh_gv       :     3 sec | \tsolution = 1.00000 | \teqconst = -0.00000\n",
      "scipy_eigh_gvd      :     4 sec | \tsolution = 1.00000 | \teqconst = -0.00000\n",
      "scipy_eigh_gvx_ss   :     2 sec | \tsolution = 1.00000 | \teqconst = -0.00000\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from nc import NormalizedCuts\n",
    "\n",
    "node = NormalizedCuts(eps=1e-8)#, bipart=args.bipart, symm_norm_L=args.symm_norm_L)\n",
    "\n",
    "\n",
    "\n",
    "for name, func in eigs_options.items(): \n",
    "    t0 = time()\n",
    "    y,_ = node.solve(A,func=func) # The output also includes context (not needed herex)\n",
    "    t = time() - t0\n",
    "    \n",
    "    y = torch.real(y)\n",
    "    y = y.reshape(1,n,n)\n",
    "    \n",
    "    \n",
    "    obj = node.objective(A,y)\n",
    "    eqconst = node.equality_constraints(A,y)\n",
    "    # Check against objetive function, should solve as close to machine precision as possible    \n",
    "    print(f\"{name:20}: {t:5.0f} sec | \\tsolution = {obj.item():1.5f} | \\teqconst = {eqconst.item():1.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Plot the speed differences in a plot\n",
    "TODO: Plot the outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d5fe071eb1c6cab15b1ad8a1b991a5b0fe2969ee0e1d70b33f4a0ef5774aa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
