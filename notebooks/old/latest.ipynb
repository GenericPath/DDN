{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f5ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4937,  0.4136,  0.7650], dtype=torch.float64)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Starting again, trying to solve it all...\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.optimize as opt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddn.pytorch.node import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NormalizedCuts(EqConstDeclarativeNode):\n",
    "    \"\"\"\n",
    "    A declarative node to embed Normalized Cuts into a Neural Network\n",
    "    \n",
    "    Normalized Cuts and Image Segmentation https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf\n",
    "    Shi, J., & Malik, J. (2000)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def objective(self, x, y):\n",
    "        \"\"\"\n",
    "        f(W,y) = y^T * (D-W) * y / y^T * D * y\n",
    "        \"\"\"\n",
    "        # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "        D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "        L = D - x\n",
    "        \n",
    "        y_t = torch.t(y)\n",
    "        \n",
    "        return torch.div(torch.matmul(torch.matmul(y_t, L),y),torch.matmul(torch.matmul(y_t,D),y))\n",
    "    \n",
    "    def equality_constraints(self, W, y):\n",
    "        \"\"\"\n",
    "        subject to y^T * D * 1 = 0\n",
    "        \"\"\"\n",
    "        # Ensure correct size and shape of y... scipy minimise flattens y         \n",
    "        N = W.size(dim=0)\n",
    "        \n",
    "        #x is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "        D = W.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "        ONE = torch.ones(N,1)   # Nx1 vector of all ones\n",
    "        \n",
    "        return torch.mm(torch.mm(torch.t(y),D), ONE)\n",
    "\n",
    "    def solve(self, W):\n",
    "        \"\"\"\n",
    "        Minimise the objective, by solving the second smallest eigenvector of laplacian\n",
    "        (D-W)*y = lambda * D * y\n",
    "        becomes D^-0.5 * (D-W) * D^-0.5 * y = lambda * y\n",
    "        \"\"\"\n",
    "        # normalised laplacian\n",
    "        D = W.sum(1).diag()\n",
    "        L = D - W\n",
    "        # Solve using torch.linalg.eigh?\n",
    "        eigval, eigvec = torch.linalg.eig(L)\n",
    "\n",
    "        val = [np.round(i.real,4) for i in eigval]\n",
    "\n",
    "        vec = []\n",
    "        for i in range(len(eigvec)):\n",
    "            v = [np.round(n.real,4) for n in eigvec[:,i]]\n",
    "            vec.append(v)\n",
    "\n",
    "        s = list(val).copy()\n",
    "        s.sort()\n",
    "\n",
    "        for i in range(len(eigval)):\n",
    "            if val[i] == s[1]:\n",
    "                second_smallest_eigval_vec = vec[i]\n",
    "                break\n",
    "        \n",
    "        return torch.tensor(second_smallest_eigval_vec), _\n",
    "    \n",
    "node = NormalizedCuts()\n",
    "x = torch.tensor([[0,1,0], [2,0,3], [0,4,0]]).double()\n",
    "y, misc = node.solve(x)\n",
    "# node.gradient(x)\n",
    "print(y)\n",
    "print(\"done\")\n",
    "# torch.cuda.set_device(gpu)\n",
    "# model = model.cuda(gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddn",
   "language": "python",
   "name": "ddn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
