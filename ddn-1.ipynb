{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Declarative Networks - Image segmentation testing\n",
    "---\n",
    "For local (cpuonly) torch install create a conda environment with\n",
    "> conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "\n",
    "> pip install -r requirements.txt\n",
    "\n",
    "> jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f039b8a1590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ4ElEQVR4nO3d/8uddR3H8derqZRpCjVibKPtBxlIkNMxkIXQxJgp2g/9sIFCEuyXFKVAtF9m/4DYDyGMqQkupaaCiGmCSgllbnOV2zTWMHYPbZMYfoPG9NUP91lMued9netc132d+83zAQfv84V778N87rrOdc65Pk4iAHV8YegBAHSLqIFiiBoohqiBYogaKOacPn6pbQ6p46yWLVs29AiL3okTJ/TRRx95rvt6iRr4PFu3bh16hEVv+/btZ72P3W+gGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoJhGUdveZPtN24ds39X3UADamzdq20sk/VLStZIulbTF9qV9DwagnSZb6vWSDiU5nOSkpMck3djvWADaahL1cklHzrg+M7rtU2xvtb3b9u6uhgMwvs6+pZVku6TtEl+9BIbUZEt9VNLKM66vGN0GYAo1ifpVSZfYXm37PEmbJT3V71gA2pp39zvJKdu3SnpO0hJJDybZ3/tkAFpp9Jo6yTOSnul5FgAd4BNlQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDGs0AFJ0rZt24YeAR1hSw0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFNVuh40PYx268vxEAAJtNkS/0rSZt6ngNAR+aNOskfJP1nAWYB0IHOvqVle6ukrV39PgDtsOwOUAxHv4FiiBoopslbWo9K+pOkNbZnbP+o/7EAtNVkLa0tCzEIgG6w+w0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxTc5RttL2i7YP2N5v+/aFGAxAO03O+31K0k+T7LV9oaQ9tp9PcqDn2QC00GTZnbeT7B39/L6kg5KW9z0YgHbGWqHD9ipJayW9Msd9LLsDTIHGUdu+QNLjku5I8t5n72fZHWA6NDr6bftczQa9M8kT/Y4EYBJNjn5b0gOSDia5t/+RAEyiyZZ6g6SbJW20vW90+V7PcwFoqcmyOy9L8gLMAqADfKIMKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWLG+pYWFta2bduGHgGLEFtqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqCYJice/KLtv9j+62jZnZ8vxGAA2mnyMdH/StqY5IPRqYJftv27JH/ueTYALTQ58WAkfTC6eu7owsn6gSnV9GT+S2zvk3RM0vNJ5lx2x/Zu27s7nhHAGBpFneTjJJdJWiFpve1vzvGY7UnWJVnX8YwAxjDW0e8kJyS9KGlTL9MAmFiTo99LbV88+vlLkq6R9EbPcwFoqcnR72WSHra9RLP/CPwmydP9jgWgrSZHv/+m2TWpASwCfKIMKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWJ6ifqKK65QEi4TXoA22FIDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBM46hHJ/R/zTYnHQSm2Dhb6tslHexrEADdaLrszgpJ10na0e84ACbVdEt9n6Q7JX1ytgecuZbW8ePHu5gNQAtNVui4XtKxJHs+73FnrqW1dOnSzgYEMJ4mW+oNkm6w/ZakxyRttP1Ir1MBaG3eqJPcnWRFklWSNkt6IclNvU8GoBXepwaKabJA3v8leUnSS71MAqATbKmBYogaKIaogWKIGiiGqIFiiBoohqiBYsZ6nxoL65577in5Z6FfbKmBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiim0cdER2cSfV/Sx5JOJVnX51AA2hvns9/fSfJub5MA6AS730AxTaOOpN/b3mN761wPYNkdYDo0jfrbSS6XdK2kH9u+6rMPYNkdYDo0ijrJ0dF/j0l6UtL6PocC0F6TBfK+bPvC0z9L+q6k1/seDEA7TY5+f13Sk7ZPP/7XSZ7tdSoArc0bdZLDkr61ALMA6ABvaQHFEDVQDFEDxRA1UAxRA8UQNVAMUQPFsOwOJLHETyVsqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKZR1LYvtr3L9hu2D9q+su/BALTT9LPfv5D0bJIf2D5P0vk9zgRgAvNGbfsiSVdJ+qEkJTkp6WS/YwFoq8nu92pJxyU9ZPs12ztG5//+FJbdAaZDk6jPkXS5pPuTrJX0oaS7Pvsglt0BpkOTqGckzSR5ZXR9l2YjBzCF5o06yTuSjtheM7rpakkHep0KQGtNj37fJmnn6Mj3YUm39DcSgEk0ijrJPknr+h0FQBf4RBlQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFzBu17TW2951xec/2HQswG4AW5j1HWZI3JV0mSbaXSDoq6cl+xwLQ1ri731dL+meSf/UxDIDJjRv1ZkmPznUHy+4A06Fx1KNzft8g6bdz3c+yO8B0GGdLfa2kvUn+3dcwACY3TtRbdJZdbwDTo1HUo6Vrr5H0RL/jAJhU02V3PpT01Z5nAdABPlEGFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFO0v0vtY9LGvfrmV+T9G7nw0yHqs+N5zWcbySZ85tTvUTdhu3dSdYNPUcfqj43ntd0YvcbKIaogWKmKertQw/Qo6rPjec1habmNTWAbkzTlhpAB4gaKGYqora9yfabtg/Zvmvoebpge6XtF20fsL3f9u1Dz9Ql20tsv2b76aFn6ZLti23vsv2G7YO2rxx6pnEN/pp6tEDAPzR7uqQZSa9K2pLkwKCDTcj2MknLkuy1faGkPZK+v9if12m2fyJpnaSvJLl+6Hm6YvthSX9MsmN0Bt3zk5wYeKyxTMOWer2kQ0kOJzkp6TFJNw4808SSvJ1k7+jn9yUdlLR82Km6YXuFpOsk7Rh6li7ZvkjSVZIekKQkJxdb0NJ0RL1c0pEzrs+oyP/8p9leJWmtpFcGHqUr90m6U9InA8/RtdWSjkt6aPTSYsfopJuLyjREXZrtCyQ9LumOJO8NPc+kbF8v6ViSPUPP0oNzJF0u6f4kayV9KGnRHeOZhqiPSlp5xvUVo9sWPdvnajbonUmqnF55g6QbbL+l2ZdKG20/MuxInZmRNJPk9B7VLs1GvqhMQ9SvSrrE9urRgYnNkp4aeKaJ2bZmX5sdTHLv0PN0JcndSVYkWaXZv6sXktw08FidSPKOpCO214xuulrSojuw2ei8331Kcsr2rZKek7RE0oNJ9g88Vhc2SLpZ0t9t7xvd9rMkzww3Ehq4TdLO0QbmsKRbBp5nbIO/pQWgW9Ow+w2gQ0QNFEPUQDFEDRRD1EAxRA0UQ9RAMf8DWK1JyZVNGrcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain PASCALVOC 2007 segmentation dataset from pytorch\n",
    "# switch False to True if not already downloaded\n",
    "%matplotlib inline\n",
    "# or notebook for interactive?\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open('./data/clean.png')\n",
    "voc = torchvision.datasets.VOCSegmentation(root='./data/voc07/', year='2007', image_set='train', download=False)\n",
    "\n",
    "# cifar10 = torchvision.datasets.CIFAR10(root='./data/cifar10/', train=True, download= False)\n",
    "# image,target = cifar10[0]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skimage\n",
    "\n",
    "# # This is a simple dissimilarity graph creation, using absolute intensity differences\n",
    "# # calculated using YUV colour space (thanks scikit-image package for doing everything)\n",
    "# def create_graph(image):\n",
    "#     # create the pixel graph with edge weights as dissimilarities\n",
    "#     yuv = skimage.color.rgb2yiq(image)\n",
    "#     (w, h) = yuv.shape[:2]\n",
    "#     edges_img = np.zeros((h, w))\n",
    "#     for i in range(yuv.shape[0]):\n",
    "#         for j in range(yuv.shape[1]):\n",
    "#             # compute edge weight for nbd pixel nodes for the node i,j\n",
    "#             for i1 in range(i-1, i+2):\n",
    "#                 for j1 in range(j-1, j+2):\n",
    "#                     if i1 == i and j1 == j: continue\n",
    "#                     if i1 >= 0 and j1 >= 0 and i1 < h and j1 < w:\n",
    "#                         wt = np.abs(yuv[i,j,0]-yuv[i1,j1,0]) # how dissimilar they are\n",
    "#                         edges_img[i1,j1] = wt\n",
    "#     return edges_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def brightness(r,g,b):\n",
    "#     return 0.2126 * r + 0.7152 * g + 0.0722 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def guassian_kernel(image):\n",
    "#     r = 5 # radius for pixel weights\n",
    "#     sigma_I = 1\n",
    "#     sigma_x = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "#     (w, h) = yuv.shape[:2]\n",
    "#     edges_img = np.zeros((h, w))\n",
    "#     for x in range(w):\n",
    "#         for y in range(h):\n",
    "#             if\n",
    "#             edges[x,y] = \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# w = create_graph(image)\n",
    "# plt.imshow(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now we can do normalised cutting from [Shi & Malik (2000)] https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf\n",
    "# w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# 4 Connected neighbours adjacency matrix (generic W)\n",
    "def create_adjacency(image):\n",
    "    r,c = image.size\n",
    "    d1 = np.tile(np.append(np.ones(c-1), [0]), r)[:-1]\n",
    "    d2 = np.ones(c*(r-1))\n",
    "    upper_diags = scipy.sparse.diags([d1, d2], [1, c])\n",
    "    return upper_diags + upper_diags.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_adjacency(image)\n",
    "print(test.shape)\n",
    "print(test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29815/2773834566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# visualise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torchvision/datasets/cityscapes.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# OLD STUFF FROM in[1]\n",
    "# voc = torchvision.datasets.VOCSegmentation(root='./data/voc07/', year='2007', image_set='train', download=False)\n",
    "# # voc = torchvision.datasets.VOCDetection(root='./data/voc07detect/', year='2007', image_set='train', download=False)\n",
    "# # each index is a tuple of (image, segmentation)\n",
    "\n",
    "# img = voc[0][0]\n",
    "# plt.imshow(img)\n",
    "\n",
    "# img.getpixel((0,0))[0]\n",
    "\n",
    "# CITYSCAPES DATASET (these images are big...)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# https://github.com/pytorch/vision/issues/2212#issuecomment-944225785\n",
    "# Compose and StandardCompose don't work together nicely?\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(100,100)),\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "train_city = torchvision.datasets.Cityscapes(root='./data/cityscapes/', split='train', mode='fine',\n",
    "                     target_type='semantic', transforms=data_transform)\n",
    "\n",
    "test_city = torchvision.datasets.Cityscapes(root='./data/cityscapes/', split='test', mode='fine',\n",
    "                     target_type='semantic', transforms=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_city, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_city, batch_size=1, shuffle=True)\n",
    "\n",
    "# visualise\n",
    "train_img = next(iter(train_loader))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c744e1a5b6aac4ad03aa092328bec4dfdace4872f6f87fd4a7c5b5fd8be1581"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
