{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor(2.)\n",
      "~~\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 5., 0.],\n",
      "        [0., 0., 4.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [2., 0., 3.],\n",
      "        [0., 4., 0.]])\n",
      "tensor([[ 3.,  0.,  0.],\n",
      "        [ 0., 10.,  0.],\n",
      "        [ 0.,  0.,  7.]])\n",
      "~~\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[10.],\n",
      "        [17.],\n",
      "        [15.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[0.,1.,0.], [2.,0.,3.], [0.,4.,0.]])\n",
    "W = A\n",
    "\n",
    "out_degree = torch.sum(W, dim=0)\n",
    "in_degree = torch.sum(W, dim=1)\n",
    "identity = torch.eye(W.size()[0])\n",
    "\n",
    "print(A.size(dim=0))\n",
    "print(A[1,0])\n",
    "\n",
    "# print(\"~~\")\n",
    "\n",
    "\n",
    "# print(W.sum(1).diag())\n",
    "\n",
    "# D = identity*in_degree + identity*out_degree - torch.diagflat(torch.diagonal(W))\n",
    "# print(W)\n",
    "# print(D)\n",
    "\n",
    "print(\"~~\")\n",
    "W = A   # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "D = W.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "ONE = torch.ones(x.size(dim=0),1)   # Nx1 vector of all ones\n",
    "\n",
    "print(type(torch.t(W)))\n",
    "print(type(torch.mm(torch.t(W), D)))\n",
    "print(type(ONE))\n",
    "\n",
    "print(ONE)\n",
    "print(torch.t(W) * D * ONE)\n",
    "\n",
    "\n",
    "\n",
    "print(torch.mm(torch.mm(torch.t(W), D), ONE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch optim solving\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import scipy.optimize as opt\n",
    "# import numpy as np\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "# from ddn.pytorch.node import *\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class NormalizedCuts(AbstractDeclarativeNode):\n",
    "#     \"\"\"\n",
    "#     A declarative node to embed Normalized Cuts into a Neural Network\n",
    "    \n",
    "#     Normalized Cuts and Image Segmentation https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf\n",
    "#     Shi, J., & Malik, J. (2000)\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def objective(self, x, y):\n",
    "#         \"\"\"\n",
    "#         f(x,y) = y^T(D-W)y / y^TDy\n",
    "#         for W = x\n",
    "#         \"\"\"\n",
    "#         W = x   # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "                \n",
    "#         D = W.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "#         ONE = torch.ones(x.size(dim=0))   # Nx1 vector of all ones\n",
    "#         L = D - W\n",
    "        \n",
    "#         return (torch.mm(torch.mm((torch.t(y),L),y))/ (torch.mm(torch.transpose(y),D),y))\n",
    "    \n",
    "#     def equality_constraints(self, x, y):\n",
    "#         \"\"\"\n",
    "#         subject to y^TD1=0\n",
    "#         \"\"\"\n",
    "#         W = x   # W is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "#         D = W.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "#         ONE = torch.ones(x.size(dim=0))   # Nx1 vector of all ones\n",
    "#         return torch.mm(torch.mm((torch.t(y),D),ONE))\n",
    "\n",
    "#     def solve(self, x):\n",
    "        \n",
    "        \n",
    "#         return y, None\n",
    "    \n",
    "#     def _run_optimisation(self, *xs, y):\n",
    "#         with torch.enable_grad():\n",
    "#             opt = torch.optim.LBFGS([y],\n",
    "#                                     lr=1.0,\n",
    "#                                     max_iter=1000,\n",
    "#                                     max_eval=None,\n",
    "#                                     tolerance_grad=1e-40,\n",
    "#                                     tolerance_change=1e-40,\n",
    "#                                     history_size=100,\n",
    "#                                     line_search_fn=\"strong_wolfe\"\n",
    "#                                     )\n",
    "#             def reevaluate():\n",
    "#                 opt.zero_grad()\n",
    "#                 f = self.objective(*xs, y=y).sum() # sum over batch elements\n",
    "#                 f.backward()\n",
    "#                 return f\n",
    "#             opt.step(reevaluate)\n",
    "#         return y\n",
    "    \n",
    "# # class Net(nn.Module):\n",
    "# #     def __init__(self):\n",
    "# #         \"\"\"\n",
    "# #         instatiate parameter\n",
    "# #         \"\"\"\n",
    "# #         super().__init__()\n",
    "    \n",
    "# # GPU ID to use\n",
    "# # gpu = 1\n",
    "\n",
    "# # Create model, set to use GPU\n",
    "# # model = Net()\n",
    "\n",
    "# node = NormalizedCuts()\n",
    "# x = torch.tensor([[0,1,0], [2,0,3], [0,4,0]])\n",
    "# y,_ = node.solve(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[-1.0000e-09,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e-09,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e-09,  0.0000e+00,  0.0000e+00]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The user-provided objective function must return a scalar value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28674/3811395740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizedCuts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28674/3811395740.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m                               \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'eq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fun'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequality_constraints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                               \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L-BFGS-B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                               options={'disp': True}) #print output\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 raise ValueError(\"The user-provided \"\n\u001b[0m\u001b[1;32m    703\u001b[0m                                  \u001b[0;34m\"objective function must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                                  \"return a scalar value.\")\n",
      "\u001b[0;31mValueError\u001b[0m: The user-provided objective function must return a scalar value."
     ]
    }
   ],
   "source": [
    "# SCIPY SOLVE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.optimize as opt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from ddn.pytorch.node import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NormalizedCuts(EqConstDeclarativeNode):\n",
    "    \"\"\"\n",
    "    A declarative node to embed Normalized Cuts into a Neural Network\n",
    "    \n",
    "    Normalized Cuts and Image Segmentation https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf\n",
    "    Shi, J., & Malik, J. (2000)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def objective(self, x, y):\n",
    "        \"\"\"\n",
    "        f(x,y) = y^T(D-W)y / y^TDy\n",
    "        for W = x\n",
    "        \"\"\"\n",
    "        # Ensure correct size and shape of y... scipy minimise flattens y         \n",
    "        N = x.size(dim=0)\n",
    "        y = torch.tensor(y).reshape(N,N)\n",
    "        \n",
    "        # x is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "        D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "        ONE = torch.ones(x.size(dim=0),1)   # Nx1 vector of all ones\n",
    "        L = D - x\n",
    "        \n",
    "        top_a = torch.mm(torch.t(y), L)\n",
    "        top_b = torch.mm(top_a, y)\n",
    "        \n",
    "        bot_a = torch.mm(torch.t(y), D)\n",
    "        bot_b = torch.mm(bot_a, y)\n",
    "        \n",
    "        \n",
    "        f = torch.div(top_b, bot_b)\n",
    "        print(f)\n",
    "        return f\n",
    "    \n",
    "    def equality_constraints(self, x, y):\n",
    "        \"\"\"\n",
    "        subject to y^TD1=0\n",
    "        \"\"\"\n",
    "        # Ensure correct size and shape of y... scipy minimise flattens y         \n",
    "        N = x.size(dim=0)\n",
    "        y = torch.tensor(y).reshape(N,N)\n",
    "        \n",
    "        #x is an NxN symmetrical matrix with W(i,j) = w_ij\n",
    "        D = x.sum(1).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "        ONE = torch.ones(N,1)   # Nx1 vector of all ones\n",
    "        \n",
    "        a = torch.mm(torch.t(y),D)\n",
    "        b = torch.mm(a,ONE)\n",
    "        return b\n",
    "\n",
    "    def solve(self, x):\n",
    "        N = x.size(dim=0)\n",
    "        x0 = torch.ones(N,N)\n",
    "        # requires scipy 1.4.1, otherwise you recieve strange errors and minimisation doesn't work\n",
    "        result = opt.minimize(lambda y: self.objective(x, y), # objective\n",
    "                              x0, # initial guess\n",
    "                              constraints={'type': 'eq', 'fun': lambda y: self.equality_constraints(x,y)}, # constraint\n",
    "                              method='L-BFGS-B',\n",
    "                              options={'disp': True}) #print output\n",
    "        \n",
    "        y = torch.tensor(result.x).reshape(N,N)\n",
    "        print(y)\n",
    "        return torch.tensor(result.x), None\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        instatiate parameter\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "    \n",
    "# GPU ID to use\n",
    "gpu = 1\n",
    "\n",
    "# Create model, set to use GPU\n",
    "model = Net()\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "node = NormalizedCuts()\n",
    "x = torch.tensor([[0,1,0], [2,0,3], [0,4,0]]).double()\n",
    "y,_ = node.solve(x)\n",
    "print(y)\n",
    "print(\"done\")\n",
    "# torch.cuda.set_device(gpu)\n",
    "# model = model.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import scipy.optimize as opt\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "# from ddn.pytorch.node import *\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # create the example node\n",
    "# class MinKL(LinEqConstDeclarativeNode):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def objective(self, x, y):\n",
    "#         \"\"\"f(x, y) = -sum x*log(y)\"\"\"\n",
    "#         return -1.0 * torch.einsum('bn,bn->b', (x, y.log()))\n",
    "    \n",
    "#     def linear_constraint_parameters(self, y):\n",
    "#         \"\"\"Ay=d ==> sum(y) = 1\"\"\"\n",
    "#         A = y.new_ones(1, y.size(-1)) # 1xm\n",
    "#         d = y.new_ones(1) # 1\n",
    "#         return A, d\n",
    "        \n",
    "#     def solve(self, x):\n",
    "#         \"\"\"Solve the constrained optimization problem using scipy's built-in minimize function.\n",
    "#         Here we initialize the solver at the uniform distribution.\n",
    "#         \"\"\"\n",
    "#         m = n = x.size(-1)\n",
    "#         u0 = np.ones((m,)) / m\n",
    "#         y = torch.zeros_like(x)\n",
    "#         # Loop over batch:\n",
    "#         for i, xi in enumerate(x):\n",
    "#             result = opt.minimize(lambda u: -1.0 * np.dot(xi.detach().numpy(), np.log(u)),\n",
    "#                                   u0,\n",
    "#                                   constraints={'type': 'eq', 'fun': lambda u: np.sum(u) - 1.0},\n",
    "#                                   bounds=opt.Bounds(1e-12, np.inf, keep_feasible=True),\n",
    "#                                   options={'maxiter': 100000, 'ftol': 1e-12})\n",
    "#             y[i, :] = torch.tensor(result.x)\n",
    "        \n",
    "#         # The solve function must always return two arguments, the solution and context (i.e., cached values needed\n",
    "#         # for computing the gradient). In the case of linearly constrained problems we do not need the dual solution\n",
    "#         # in computing the gradient so we return None for context.\n",
    "#         return y, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[0.869236   0.80243564 0.5088969  0.92334807 0.5429707 ]\n",
      "Expected output:\n",
      "[0.23835011 0.22003302 0.1395428  0.25318798 0.14888607]\n",
      "Actual output:\n",
      "[0.23835011 0.22003302 0.1395428  0.25318798 0.14888607]\n"
     ]
    }
   ],
   "source": [
    "# node = MinKL()\n",
    "# x = torch.rand(1, 5)\n",
    "# print(\"Input:\\n{}\".format(x.squeeze().numpy()))\n",
    "# print(\"Expected output:\\n{}\".format((x / x.sum(dim=-1, keepdim=True)).squeeze().numpy()))\n",
    "\n",
    "# y, _ = node.solve(x)\n",
    "# print(\"Actual output:\\n{}\".format(y.squeeze().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
