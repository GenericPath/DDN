{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e7a8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.],\n",
      "        [10.]], requires_grad=True)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ddn.pytorch.node import *\n",
    "\n",
    "def f(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "# L-BFGS\n",
    "x_lbfgs = 10*torch.ones(2, 2)\n",
    "x_lbfgs.requires_grad = True\n",
    "\n",
    "optimizer = optim.LBFGS([x_lbfgs],\n",
    "                        history_size=10,\n",
    "                        max_iter=4,\n",
    "                        line_search_fn=\"strong_wolfe\")\n",
    "print(x_lbfgs)\n",
    "\n",
    "h_lbfgs = []\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    objective = f(x_lbfgs)\n",
    "    objective.backward()\n",
    "    optimizer.step(lambda: f(x_lbfgs))\n",
    "    h_lbfgs.append(objective.item())\n",
    "    \n",
    "print(x_lbfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4a6064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48599.46576081  38373.48733538  42606.22737027  64382.48332668\n",
      "   75880.33838495]\n",
      " [ 38373.48733538 108485.22082652  77075.26240585  56840.58479613\n",
      "   88266.94769604]\n",
      " [ 42606.22737027  77075.26240585 120540.05088788  76841.17040939\n",
      "  141603.78912639]\n",
      " [ 64382.48332668  56840.58479613  76841.17040939 113480.37148607\n",
      "  134061.21810102]\n",
      " [ 75880.33838495  88266.94769604 141603.78912639 134061.21810102\n",
      "  198665.55956067]]\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 4., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 3.]], dtype=torch.float64)\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 4., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 3.]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# This semi works, but the solution is wrong and the gradient crashes half way\n",
    "\n",
    "import autograd.numpy as np\n",
    "import torch\n",
    "from autograd import grad, jacobian\n",
    "\n",
    "def gradient(f, x, y):\n",
    "    fY = grad(f,1)\n",
    "    fYY = jacobian(fY, 1)\n",
    "    fXY = jacobian(fY, 0)\n",
    "    \n",
    "    return -1.0 * np.linalg.solve(fYY(x,y), fXY(x,y))\n",
    "\n",
    "n = 5\n",
    "M = np.random.uniform(0,255,(n,n))\n",
    "symm = M@M.T\n",
    "# test for symmetry\n",
    "print(symm)\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "x = torch.tensor(symm, requires_grad=True)\n",
    "\n",
    "D = x.sum(0).diag() # D is an NxN diagonal matrix with d on diagonal, for d(i) = sum_j(w(i,j))\n",
    "ONE = torch.ones(x.size(dim=0),1)   # Nx1 vector of all ones\n",
    "L = D - x\n",
    "\n",
    "# L.backward(x)\n",
    "\n",
    "L.t()\n",
    "\n",
    "\n",
    "x = torch.tensor([[0,1,0,0], [1,0,0,3], [0,0,0,0], [0,3,0,0]]).double()\n",
    "D = x.sum(0).diag()\n",
    "print(D)\n",
    "\n",
    "x1 = torch.tensor([[[0,1,0,0], [1,0,0,3], [0,0,0,0], [0,3,0,0]]]).double()\n",
    "D = torch.einsum('bij->bj', x1)\n",
    "d1, d2 = D.size()\n",
    "D = torch.diag_embed(D)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6393a0f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0.],\n",
      "         [1., 0., 0., 3.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 3., 0., 0.]]])\n",
      "torch.Size([1, 4, 4])\n",
      "tensor([[[0.6662, 0.3332, 1.1314, 0.1954],\n",
      "         [0.6891, 0.7011, 0.8247, 0.1113],\n",
      "         [0.6618, 0.0142, 0.4685, 0.2312],\n",
      "         [0.8381, 0.2516, 0.2792, 0.9572]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26665/2748434225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/gwales/DDN/ddn/pytorch/node.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, y, v, ctx, *xs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfYY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfXY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_objective_derivatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_optimality_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/DDN/ddn/pytorch/node.py\u001b[0m in \u001b[0;36m_get_objective_derivatives\u001b[0;34m(self, xs, y)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Compute partial derivative of f wrt y at (xs,y):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mfY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mfY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if fY is independent of y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/gwales/anaconda3/envs/ddn/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ddn.pytorch.node import *\n",
    "\n",
    "# class NormalizedCuts(EqConstDeclarativeNode):\n",
    "class NormalizedCuts(AbstractDeclarativeNode):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def general_eigen(self, A, y):\n",
    "        \"\"\" f = y^T A y \"\"\"\n",
    "        \n",
    "        # Batch         \n",
    "        yT = torch.einsum('bij->bji', y)\n",
    "        # Batch matrix multiplication\n",
    "        return torch.einsum('bij,bjk->bik', torch.einsum('bij,bjk->bik', yT, A), y)\n",
    "        \n",
    "        # For single problem...        \n",
    "        # return torch.matmul(torch.matmul(y.t()), A), y)\n",
    "    \n",
    "    def objective(self, x, y):\n",
    "        \"\"\" f(x,y) = y^T (D-W) y \"\"\"\n",
    "        D = torch.einsum('bij->bj', x1)\n",
    "        D = torch.diag_embed(D)\n",
    "        L = D - x # Laplacian matrix\n",
    "        return self.general_eigen(L, y)\n",
    "        \n",
    "#     def equality_constraints(self, x, y):\n",
    "#         \"\"\" h(x,y) = y^T y = 1 \"\"\"\n",
    "#         return torch.matmul(y.t(), y) - 1\n",
    "        \n",
    "    def solve(self, x):\n",
    "        x.detach()\n",
    "        y = torch.rand_like(x, requires_grad=True)\n",
    "        y = self._run_optimisation( x, y=y)\n",
    "        return y.detach(), None\n",
    "    \n",
    "    def _run_optimisation(self, *xs, y):\n",
    "            with torch.enable_grad():\n",
    "                opt = torch.optim.LBFGS([y],\n",
    "                                        lr=1.0,\n",
    "                                        max_iter=1000,\n",
    "                                        max_eval=None,\n",
    "                                        tolerance_grad=1e-40,\n",
    "                                        tolerance_change=1e-40,\n",
    "                                        history_size=100,\n",
    "                                        line_search_fn=\"strong_wolfe\"\n",
    "                                        )\n",
    "                def reevaluate():\n",
    "                    opt.zero_grad()\n",
    "                    f = self.objective(*xs, y=y).sum() # sum over batch elements\n",
    "                    f.backward()\n",
    "                    return f\n",
    "                opt.step(reevaluate)\n",
    "            return y\n",
    "        \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "node = NormalizedCuts()\n",
    "x = torch.tensor([[[0,1,0,0], [1,0,0,3], [0,0,0,0], [0,3,0,0]]]).double()\n",
    "print(x)\n",
    "print(x.size())\n",
    "y,_ = node.solve(x)\n",
    "print(y)\n",
    "node.gradient(x, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
